import time
import threading
import pandas as pd
import streamlit as st
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
import streamlit as st
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.service import Service
import numpy as np
from time import sleep
import threading
import nltk
import csv
import io
import spacy
import re
import num2words
from nltk import ngrams
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from langchain.chat_models import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI
import google.generativeai as genai
from bertopic import BERTopic
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline
#from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
# T√©l√©charger les ressources n√©cessaires pour NLTK
nltk.download('wordnet')
nltk.download('punkt')

# Chargement du mod√®le de langue fran√ßais
nlp = spacy.load("fr_core_news_sm")

# Variable globale pour contr√¥ler le scraping
scraping_active = False

# Fonction pour g√©n√©rer des N-grams

def scrape_youtube_comments(url, file_name, chromedriver_path):
    # Configuration de Selenium pour Chrome
    options = Options()
    options.add_argument("--headless")  # Ex√©cuter sans fen√™tre du navigateur
    service = Service(chromedriver_path)

    # Lancer le navigateur avec chromedriver
    driver = webdriver.Chrome(service=service, options=options)

    try:
        # Acc√©der √† la vid√©o YouTube
        driver.get(url)
        time.sleep(2)  # Attendre que la page se charge

        # Faire d√©filer la page pour charger les commentaires
        body = driver.find_element(By.TAG_NAME, 'body')
        for _ in range(10):  # Faire d√©filer 10 fois pour charger plus de commentaires
            body.send_keys(Keys.PAGE_DOWN)
            time.sleep(1)

        # Extraire les commentaires
        comments = []
        comment_elements = driver.find_elements(By.CSS_SELECTOR, "ytd-comment-thread-renderer #content-text")

        for comment_element in comment_elements:
            comments.append(comment_element.text)

        # Sauvegarder les commentaires dans un fichier CSV
        df = pd.DataFrame(comments, columns=["Commentaire"])
        df.to_csv(f"{file_name}.csv", index=False)

        st.success(f"Scraping termin√©. {len(comments)} commentaires extraits.")

    except Exception as e:
        st.error(f"Une erreur s'est produite : {e}")

    finally:
        driver.quit()  # Fermer le navigateur

# Fonction principale de l'interface Streamlit
def generate_ngrams(text, n=2):
    tokens = text.split()
    ngrams_list = list(ngrams(tokens, n))
    ngram_freq = Counter(ngrams_list)
    return ngram_freq

# Visualiser les r√©sultats des N-grams
def plot_ngrams(ngram_freq, title="N-grams Frequency", top_n=10):
    most_common = ngram_freq.most_common(top_n)
    ngrams_labels = [" ".join(ngram) for ngram, _ in most_common]
    frequencies = [freq for _, freq in most_common]

    # Create figure and axes
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(ngrams_labels, frequencies, color='skyblue')

    # Set x-ticks and their labels
    ax.set_xticks(range(len(ngrams_labels)))  # Set the tick positions
    ax.set_xticklabels(ngrams_labels, rotation=45, ha='right')  # Set the tick labels

    ax.set_title(title, fontsize=16)
    ax.set_xlabel("N-grams", fontsize=12)
    ax.set_ylabel("Fr√©quence", fontsize=12)
    
    st.pyplot(fig)

# Fonction pour g√©n√©rer un nuage de mots global
def generate_global_wordcloud(text):
    wordcloud = WordCloud(width=800, height=400).generate(text)
    return wordcloud

# Nuage de mots pour les N-grams
def plot_wordcloud(ngram_freq, title="WordCloud - N-grams"):
    wordcloud_data = {" ".join(ngram): freq for ngram, freq in ngram_freq.items()}
    wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(wordcloud_data)

    # Create figure and axes
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.axis('off')
    ax.set_title(title, fontsize=16)
    
    st.pyplot(fig)

# Fonction pour remplacer les chiffres par des mots en fran√ßais
def replace_numbers_with_words(text):
    return re.sub(r"\d+", lambda x: num2words.num2words(x.group(), lang="fr"), text)

# Fonction de traitement de texte
def text_processing(text, lemmatize=False):
    text = replace_numbers_with_words(text)
    text = text.lower()
    
    # Remove unwanted characters (punctuation, special characters)
    text = re.sub(r'[^\w\s]', '', text)  # Removes punctuation and special characters
    text = re.sub(r'\d+', '', text)  # Optionally remove numbers if not required
    
    doc = nlp(text)
    tokens = [token.text for token in doc]

    # Liste de mots √† ignorer, incluant cameroun et camerounais
    stop_Wd = [
        "au", "aux", "avec", "ce", "ces", "dans", "de", "des", "du", "elle", "en", "et", "eux", "il", "je",
        "la", "le", "leur", "lui", "ma", "mais", "me", "m√™me", "mes", "moi", "mon", "ne", "nos", "notre",
        "nous", "on", "ou", "par", "pas", "pour", "qu", "que", "qui", "sa", "se", "ses", "son", "sur",
        "ta", "te", "tes", "toi", "ton", "tu", "un", "une", "vos", "votre", "vous", "c", "d", "j", "l",
        "√†", "m", "n", "s", "t", "y", "√©t√©", "√©t√©e", "√©t√©es", "√©t√©s", "√©tant", "√©tante", "√©tants", "√©tantes",
        "suis", "es", "est", "sommes", "√™tes", "sont", "serai", "seras", "sera", "serons", "serez", "seront",
        "serais", "serait", "serions", "seriez", "seraient", "√©tais", "√©tait", "√©tions", "√©tiez", "√©taient",
        "fus", "fut", "f√ªmes", "f√ªtes", "furent", "sois", "soit", "soyons", "soyez", "soient", "fusse",
        "fusses", "f√ªt", "fussions", "fussiez", "fussent",
        "le", "mr", "qu", "est", "c'est", "juste", "master", "class", "toujours", "vraiment", "si", "quand",
        "jai", "j'ai", "beaucoup", "d√©j√†", "deja", "nest", "n'est", "franchement", "cet", "vois", "ceux",
        "ici", "mme", "quil", "qu'il", "trs", "cette", "tr√®s", "trop", "cest", "c'est", "comme", "tout",
        "plus", "bien", "faire", "aussi", "fait", "peut", "tre", "quel", "sans", "autre", "donc", "tous",
        "faut", "peu", "dit", "avoir", "non", "fois", "ans", "alors", "sont", "peu", "peux", "peut",
        "cher", "chers", "mesdames", "messieurs", "chaque", "ensemble", "ici", "aujourd'hui", "citoyens",
        "nigriens", "nigerien", "vingt", "cinq", "quatre", "ann√©e", "cameroun", "camerounais", "naja", "tv"
    ]
    
    stop_words = spacy.lang.fr.stop_words.STOP_WORDS
    stop_words.update(stop_Wd)

    tokens = [token for token in tokens if token not in stop_words and token.strip() != '']
    
    if lemmatize:
        tokens = [nlp(token)[0].lemma_ for token in tokens]

    text_propre = " ".join(tokens)
    return text_propre

# Page d'accueil
def home():
    st.markdown("<h1 style='text-align: center; color: #4CAF50;'>Projet de Webscraping</h1>", unsafe_allow_html=True)
    st.markdown("<h2 style='text-align: center; color: #333;'>Th√®me : Analyse des tendances des r√©seaux sociaux</h2>", unsafe_allow_html=True)
    st.markdown("<h3 style='text-align: center; color: #007BFF;'>Concepteurs :</h3>", unsafe_allow_html=True)
    st.markdown("<h4 style='text-align: center; color: #555;'>Bognare Kale Evariste</h4>", unsafe_allow_html=True)
    st.markdown("<h4 style='text-align: center; color: #555;'>Dzogoung Talla Arielle Lareine</h4>", unsafe_allow_html=True)
    st.markdown("<h4 style='text-align: center; color: #555;'>Managa Previs</h4>", unsafe_allow_html=True)
    st.markdown("<h5 style='text-align: center; color: #888;'>Sous la supervision de Mr Serge</h5>", unsafe_allow_html=True)

# Fonction pour scraper les commentaires YouTube

# Page de scraping
def scraping():
    global scraping_active
    st.title("üì• Scraping des Commentaires")
    st.write("Cette page vous permet de scraper des commentaires depuis YouTube ou Facebook.")

    # Entr√©e de l'URL et de la plateforme
    url = st.text_input("Entrez l'URL de la vid√©o YouTube ou du post Facebook:")
    platform = st.selectbox("Choisissez la plateforme:", ["YouTube", "Facebook"])
    file_name = st.text_input("Nom du fichier (sans extension):", "comments")

    if platform == "YouTube":
        chromedriver_path = st.text_input("Entrez le chemin complet vers chromedriver.exe:")

    if st.button("Scraper"):
        scraping_active = True
        if platform == "YouTube":
            if chromedriver_path and os.path.exists(chromedriver_path):
                # Lancer le scraping dans un thread s√©par√©
                threading.Thread(target=scrape_youtube_comments, args=(url, file_name, chromedriver_path)).start()
                st.success("Scraping en cours...")
            else:
                st.error("Le chemin vers chromedriver.exe est invalide ou le fichier n'existe pas.")
        elif platform == "Facebook":
            st.warning("Le scraping de Facebook n√©cessite une m√©thode sp√©cifique et n'est pas impl√©ment√©.")

    if st.button("Arr√™ter"):
        scraping_active = False
        st.success("Scraping arr√™t√©.")

    if st.button("T√©l√©charger le fichier"):
        if os.path.exists(f"{file_name}.csv"):
            with open(f"{file_name}.csv", "rb") as f:
                st.download_button("T√©l√©charger", f, file_name=f"{file_name}.csv")
        else:
            st.warning("Aucun fichier √† t√©l√©charger.")

# Page de pr√©traitement
def preprocessing():
    st.title("üîÑ Pr√©traitement")
    st.write("Cette page est d√©di√©e au pr√©traitement des donn√©es.")

    uploaded_file = st.file_uploader("Choisissez un fichier CSV", type="csv")
    
    if uploaded_file is not None:
        # Charger les donn√©es une seule fois
        if 'commentaire' not in st.session_state:
            file_content = uploaded_file.getvalue().decode('utf-8')
            file_like = io.StringIO(file_content)

            liste_comments = []
            lecteur_csv = csv.reader(file_like)
            
            for ligne in lecteur_csv:
                if any(ligne):
                    liste_comments.append(ligne)

            liste_simple = [element[0] for element in liste_comments]
            liste_filtr√©e = [element for element in liste_simple if element != "0"]

            st.session_state.commentaire = pd.DataFrame(liste_filtr√©e, columns=['Commentaire'])
        
        st.write("Aper√ßu des donn√©es charg√©es :")
        st.dataframe(st.session_state.commentaire.head())

        lemmatize = st.radio("Voulez-vous effectuer la lemmatisation des commentaires ?", ('Oui', 'Non'))

        if st.button("Nettoyer les donn√©es"):
            st.write("Traitement en cours...")
            st.session_state.commentaire['Commentaire'] = st.session_state.commentaire['Commentaire'].apply(
                lambda x: text_processing(x, lemmatize=lemmatize == 'Oui')
            )
            st.write("Donn√©es nettoy√©es :")
            st.dataframe(st.session_state.commentaire.head())

            # Stocker les commentaires nettoy√©s dans session_state
            st.session_state.comments_list = st.session_state.commentaire["Commentaire"].tolist()

        # Afficher les options pour les N-grams et les nuages de mots
        if 'comments_list' in st.session_state:
            n_choice = st.radio("Choisissez le type de N-grams :", ('Bigrams', 'Trigrams'))

            # D√©finir la valeur de n en fonction du choix de l'utilisateur
            n_value = 2 if n_choice == 'Bigrams' else 3

            if st.button("G√©n√©rer N-grams"):
                ngram_freq = generate_ngrams(" ".join(st.session_state.comments_list), n=n_value)

                # Afficher les N-grams sous forme de graphique
                plot_ngrams(ngram_freq, title=f"Top {n_value}-grams Frequency")

                # G√©n√©rer et afficher le nuage de mots pour les N-grams
                plot_wordcloud(ngram_freq, title=f"Nuage de mots - {n_value}-grams")

            # G√©n√©rer et afficher le nuage de mots global
            if st.button("G√©n√©rer le nuage de mots global"):
                global_wordcloud = generate_global_wordcloud(" ".join(st.session_state.comments_list))
                st.subheader("Nuage de mots global")
                fig, ax = plt.subplots(figsize=(10, 5))
                ax.imshow(global_wordcloud, interpolation='bilinear')
                ax.axis('off')
                st.pyplot(fig)

            # T√©l√©chargement du fichier nettoy√©
            csv_output = st.session_state.commentaire.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="T√©l√©charger le fichier CSV nettoy√©",
                data=csv_output,
                file_name='commentaires_nettoyes.csv',
                mime='text/csv',
            )


# Ajouter les colonnes sentiment et classement
def ajouter_colonnes_sentiment_classement(df):
    """
    Ajoute deux colonnes au DataFrame :
    - 'sentiment' : 'positif' si le label est 'joy' ou 'love', sinon 'n√©gatif'.
    - 'classement' : 'douteux' si score < 0.5, 'acceptable' si 0.5 <= score < 0.7, 'bon' si score >= 0.7.
    """
    # Colonne 'sentiment'
    df['sentiment'] = df['label'].apply(lambda x: 'positif' if x in ['joy', 'love'] else 'n√©gatif')
    
    # Colonne 'classement'
    df['classement'] = df['score'].apply(
        lambda x: 'douteux' if x < 0.5 else ('acceptable' if x < 0.7 else 'bon')
    )
    
    return df

# Page d'analyse des sentiments

def sentiment_analysis():
    st.title("üîç Analyse des Sentiments")
    
    
    # Demande de chemin pour enregistrer le mod√®le
    model_path = st.text_input("Entrez le chemin pour enregistrer le mod√®le (ex: C:/Users/FTAB TECH/tp4_nlp/modele):")

    if st.button("T√©l√©charger et enregistrer le mod√®le"):
        try:
            # Charger le mod√®le et le tokenizer
            model = AutoModelForSequenceClassification.from_pretrained('bhadresh-savani/bert-base-uncased-emotion')
            tokenizer = AutoTokenizer.from_pretrained('bhadresh-savani/bert-base-uncased-emotion')

            # Enregistrer le mod√®le et le tokenizer
            model.save_pretrained(model_path)
            tokenizer.save_pretrained(model_path)
            st.success("Mod√®le et tokenizer t√©l√©charg√©s et enregistr√©s avec succ√®s !")
        except Exception as e:
            st.error(f"Erreur lors du t√©l√©chargement du mod√®le : {e}")

    # File uploader for CSV file containing comments
    uploaded_file = st.file_uploader("T√©l√©chargez votre fichier CSV contenant les commentaires pour l'analyse :", type="csv")
    
    if st.button("Charger le mod√®le et analyser les sentiments"):
        if uploaded_file is not None:
            try:
                # Charger le mod√®le et le tokenizer depuis le chemin sp√©cifi√©
                model = AutoModelForSequenceClassification.from_pretrained(model_path)
                tokenizer = AutoTokenizer.from_pretrained(model_path)

                # Cr√©er un pipeline pour l'analyse de texte
                classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)

                # Lire le fichier CSV dans un DataFrame
                commentaires = pd.read_csv(uploaded_file)

                if 'Commentaire' in commentaires.columns:
                    # Appliquer le mod√®le aux commentaires
                    commentaires['Commentaire'] = commentaires['Commentaire'].fillna("")  # Remplacer NaN par des cha√Ænes vides
                    commentaires['Commentaire'] = commentaires['Commentaire'].astype(str)  # S'assurer que la colonne est de type string

                    # Appliquer la fonction √† chaque ligne du DataFrame
                    def analyser_emotion_batch(textes):
                        resultats = classifier(textes, truncation=True, max_length=512)
                        return [(res['label'], res['score']) for res in resultats]

                    # Appliquer la fonction √† chaque ligne du DataFrame
                    resultats = analyser_emotion_batch(commentaires['Commentaire'].tolist())  # Passer les commentaires en liste

                    # Cr√©er un DataFrame avec les r√©sultats
                    sentiments_df = pd.DataFrame(resultats, columns=['label', 'score'])

                    # Ajouter les r√©sultats au DataFrame original
                    commentaires[['label', 'score']] = sentiments_df[['label', 'score']]

                    # Ajouter les colonnes 'sentiment' et 'classement'
                    commentaires = ajouter_colonnes_sentiment_classement(commentaires)

                    # Stocker les r√©sultats dans st.session_state
                    st.session_state.commentaires = commentaires

                    # Afficher le DataFrame avec les √©motions et les scores
                    st.write("R√©sultats de l'analyse des sentiments :")
                    st.dataframe(commentaires.head())  # Afficher le DataFrame avec les r√©sultats

                else:
                    st.error("Le fichier CSV doit contenir une colonne nomm√©e 'Commentaire'.")
            except Exception as e:
                st.error(f"Erreur lors de l'analyse des sentiments : {e}")
        else:
            st.warning("Veuillez d'abord t√©l√©charger votre fichier CSV.")

    # Afficher les visualisations si les r√©sultats sont disponibles
    if 'commentaires' in st.session_state:
        commentaires = st.session_state.commentaires

        if st.button("Afficher la r√©partition des sentiments"):
            sns.set_style("whitegrid")
            plt.figure(figsize=(10, 5))
            sns.countplot(x=commentaires['sentiment'], palette='viridis')
            plt.title("R√©partition des sentiments (positif/n√©gatif)")
            plt.ylabel("Nombre d'occurrences")
            plt.xlabel("Sentiment")
            st.pyplot(plt)

        if st.button("Afficher la r√©partition des emotions"):
            sns.set_style("whitegrid")
            plt.figure(figsize=(10, 5))
            sns.countplot(x=commentaires['label'], palette='viridis')
            plt.title("R√©partition des emotions")
            plt.ylabel("Nombre d'occurrences")
            plt.xlabel("Emotions")
            st.pyplot(plt)

        if st.button("Afficher la r√©partition des classements"):
            sns.set_style("whitegrid")
            plt.figure(figsize=(10, 5))
            sns.countplot(x=commentaires['classement'], palette='viridis')
            plt.title("R√©partition des classements (douteux, acceptable, bon)")
            plt.ylabel("Nombre d'occurrences")
            plt.xlabel("Classement")
            st.pyplot(plt)

    def generer_nuage_mots(text, titre):
        if text.strip():  # V√©rifier si le texte n'est pas vide
            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
            plt.figure(figsize=(10, 5))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.title(titre)
            plt.axis('off')
            st.pyplot(plt)  # Utiliser st.pyplot() pour afficher dans Streamlit
        else:
            st.warning(f"Aucun mot √† afficher pour le nuage de mots de l'√©motion '{titre}'.")


        
    
    if 'commentaires' in st.session_state:
        commentaires = st.session_state.commentaires
        
        

        
            # Ajouter les r√©sultats du mod√®le √† chaque commentaire (ici, un mod√®le fictif est utilis√© pour l'exemple)
            # Dans la vraie impl√©mentation, vous appliquez le mod√®le comme dans votre code pr√©c√©dent
            # Par exemple : commentaires[['label', 'score']] = r√©sultats obtenus avec votre pipeline de classification

            # Exemples d'√©motions bas√©es sur des labels fictifs, remplacer par les r√©sultats de votre mod√®le
            # Simulation des labels
        #commentaires['label'] = ['anger', 'joy', 'love', 'fear', 'sadness'] * (len(commentaires) // 5)  # Exemple

            # S√©lectionner les commentaires par √©motion
        df_anger = commentaires[commentaires['label'] == 'anger']['Commentaire']
        df_joy = commentaires[commentaires['label'] == 'joy']['Commentaire']
        df_love = commentaires[commentaires['label'] == 'love']['Commentaire']
        df_fear = commentaires[commentaires['label'] == 'fear']['Commentaire']
        df_sadness = commentaires[commentaires['label'] == 'sadness']['Commentaire']

            # Cr√©er des textes √† partir des commentaires de chaque √©motion
        texte_anger = ' '.join(df_anger)
        texte_joy = ' '.join(df_joy)
        texte_love = ' '.join(df_love)
        texte_fear = ' '.join(df_fear)
        texte_sadness = ' '.join(df_sadness)

            # Liste d√©roulante pour choisir l'√©motion √† afficher
        emotion_choisie = st.selectbox("Choisissez une √©motion pour afficher le nuage de mots :", 
                                          ["anger", "joy", "love", "fear", "sadness"])

            # Afficher le nuage de mots pour l'√©motion s√©lectionn√©e
        if st.button("G√©n√©rer le nuage de mots"):
                if emotion_choisie == "anger":
                    generer_nuage_mots(texte_anger, "Nuage de mots pour les commentaires 'anger'")
                elif emotion_choisie == "joy":
                    generer_nuage_mots(texte_joy, "Nuage de mots pour les commentaires 'joy'")
                elif emotion_choisie == "love":
                    generer_nuage_mots(texte_love, "Nuage de mots pour les commentaires 'love'")
                elif emotion_choisie == "fear":
                    generer_nuage_mots(texte_fear, "Nuage de mots pour les commentaires 'fear'")
                elif emotion_choisie == "sadness":
                    generer_nuage_mots(texte_sadness, "Nuage de mots pour les commentaires 'sadness'")

        
        
@st.cache_data
def load_and_preprocess_data(uploaded_file):
    """
    Charge un fichier CSV et pr√©traite les commentaires.
    """
    try:
        df = pd.read_csv(uploaded_file)
        if 'Commentaire' not in df.columns:
            st.error("Le fichier CSV doit contenir une colonne nomm√©e 'Commentaire'.")
            return None
        comments = df['Commentaire'].dropna().tolist()  # Supprime les valeurs manquantes
        comments = [comment for comment in comments if comment]  # Nettoie chaque commentaire
        return comments
    except Exception as e:
        st.error(f"Erreur lors de la lecture du fichier CSV : {e}")
        return None

# Fonction pour g√©n√©rer les embeddings
@st.cache_resource
def generate_embeddings(comments):
    """
    G√©n√®re des embeddings pour les commentaires √† l'aide de SentenceTransformer.
    """
    try:
        model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        embeddings = model.encode(comments)  # Encode les commentaires en embeddings
        return embeddings
    except Exception as e:
        st.error(f"Erreur lors de la g√©n√©ration des embeddings : {e}")
        return None

# Fonction pour appliquer PCA
@st.cache_resource
def apply_pca(embeddings, n_components=2):
    """
    Applique une r√©duction de dimensionnalit√© avec PCA.
    """
    try:
        pca = PCA(n_components=n_components)
        embeddings_pca = pca.fit_transform(embeddings)  # Applique PCA aux embeddings
        return embeddings_pca
    except Exception as e:
        st.error(f"Erreur lors de l'application de PCA : {e}")
        return None

# Fonction pour g√©n√©rer les titres des th√®mes avec LangChain

# Fonction pour g√©n√©rer les titres des th√®mes avec LangChain
# Fonction pour g√©n√©rer les titres des th√®mes avec LangChain

# Fonction pour g√©n√©rer les titres des th√®mes avec Google Gemini
def generate_topic_titles(topic_info, num_topics, gemini_api_key):
    """
    G√©n√®re des titres pour les th√®mes √† l'aide de Google Gemini.
    """
    try:
        # Configurer l'API Gemini
        genai.configure(api_key=gemini_api_key)
        model = genai.GenerativeModel('gemini-pro')

        prompt = """
        Tu es un agent qui permet de donner un titre √† un ensemble de topics que l'utilisateur te donnera. Donne uniquement le titre comme r√©ponse.

        Topics :
        {topics}
        """
        titles = []
        for i in range(1, num_topics + 1):
            # V√©rifiez si la colonne 'Representative_Docs' existe, sinon utilisez une autre colonne
            if 'Representative_Docs' in topic_info.columns:
                topics_text = " ".join(topic_info['Representative_Docs'].iloc[i - 1])
            elif 'Representation' in topic_info.columns:
                topics_text = " ".join(topic_info['Representation'].iloc[i - 1])
            else:
                st.error("Colonne 'Representative_Docs' ou 'Representation' introuvable dans topic_info.")
                return None

            final_prompt = prompt.format(topics=topics_text)
            try:
                response = model.generate_content(final_prompt)  # Utilisation de Gemini
                titles.append(response.text)
            except Exception as e:
                st.error(f"Erreur lors de la g√©n√©ration des titres des th√®mes : {e}")
                return None
        return titles
    except Exception as e:
        st.error(f"Erreur lors de la g√©n√©ration des titres des th√®mes : {e}")
        return None

# Page de Topic Modeling
def topic_modeling():
    st.title("üß† Mod√©lisation des Th√®mes")

    # √âtape 1 : T√©l√©chargement du fichier CSV
    uploaded_file = st.file_uploader("T√©l√©chargez votre fichier CSV contenant les commentaires :", type="csv")
    if uploaded_file is not None:
        if st.button("Valider le fichier CSV"):
            comments = load_and_preprocess_data(uploaded_file)
            if comments:
                st.session_state.comments = comments  # Sauvegarde les commentaires dans session_state
                st.success("Fichier CSV valid√© et commentaires pr√©trait√©s avec succ√®s !")

    # √âtape 2 : G√©n√©ration des embeddings
    if 'comments' in st.session_state:
        if st.button("G√©n√©rer les embeddings"):
            embeddings = generate_embeddings(st.session_state.comments)
            if embeddings is not None:
                st.session_state.embeddings = embeddings  # Sauvegarde les embeddings dans session_state
                st.success("Embeddings g√©n√©r√©s avec succ√®s !")

    # √âtape 3 : Param√©trage et application de PCA
    if 'embeddings' in st.session_state:
        st.sidebar.header("Param√®tres de mod√©lisation")
        min_topic_size = st.sidebar.slider("Taille minimale des th√®mes", min_value=2, max_value=20, value=5)
        n_components = st.sidebar.slider("Nombre de composantes pour PCA", min_value=2, max_value=10, value=2)
        num_topics = st.sidebar.number_input("Nombre de th√®mes √† afficher", min_value=1, max_value=50, value=5)

        if st.button("Appliquer PCA et mod√©liser les th√®mes"):
            embeddings_pca = apply_pca(st.session_state.embeddings, n_components=n_components)
            if embeddings_pca is not None:
                st.session_state.embeddings_pca = embeddings_pca  # Sauvegarde les embeddings PCA dans session_state

                # Initialisation de BERTopic
                stopwords_fr = ["le", "la", "les", "un", "une", "des", "du", "de", "dans", "sur", 
                               "pour", "avec", "par", "au", "aux", "ce", "ces", "√ßa", "elle", 
                               "il", "ils", "elles", "nous", "vous", "on", "en", "et", "ou", "mais"]
                vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords_fr)
                topic_model = BERTopic(language="french", vectorizer_model=vectorizer_model, min_topic_size=min_topic_size)

                # Ajuster le mod√®le BERTopic
                topics, probs = topic_model.fit_transform(st.session_state.comments, st.session_state.embeddings_pca)
                st.session_state.topic_model = topic_model  # Sauvegarde le mod√®le dans session_state
                st.success("PCA appliqu√©e et th√®mes mod√©lis√©s avec succ√®s !")

                # Afficher les th√®mes g√©n√©r√©s
                topic_info = topic_model.get_topic_info()
                st.write("Informations sur les th√®mes g√©n√©r√©s :")
                st.dataframe(topic_info)

                # G√©n√©ration des titres des th√®mes avec Google Gemini
                gemini_api_key = st.text_input("Entrez votre cl√© API Google Gemini :", type="password")
                if gemini_api_key and st.button("G√©n√©rer les titres des th√®mes"):
                    titles = generate_topic_titles(topic_info, num_topics, gemini_api_key)
                    if titles:
                        for i, title in enumerate(titles):
                            st.write(f"**Titre pour le th√®me {i+1}:** {title}")















    # √âtape 4 : Affichage des th√®mes et visualisation
    if 'topic_model' in st.session_state:
        topic_info = st.session_state.topic_model.get_topic_info()
        st.write("Informations sur les th√®mes g√©n√©r√©s :")
        st.dataframe(topic_info)

        if len(topic_info) > 1:  # V√©rifie si des th√®mes ont √©t√© g√©n√©r√©s
            
            num_topics = min(num_topics, len(topic_info) - 1)  # Limite le nombre de th√®mes affich√©s

            # Visualisation des th√®mes avec Plotly
            st.subheader("Visualisation des th√®mes")
            fig = st.session_state.topic_model.visualize_barchart(topics=range(1,num_topics))
            st.plotly_chart(fig)

            # G√©n√©ration des titres des th√®mes avec LangChain
            openai_api_key = st.text_input("Entrez votre cl√© API OpenAI :", type="password")
            if openai_api_key and st.button("G√©n√©rer les titres des th√®mes"):
                titles = generate_topic_titles(topic_info, num_topics, openai_api_key)
                if titles:
                    for i, title in enumerate(titles):
                        st.write(f"**Titre pour le th√®me {i+1}:** {title}")
        else:
            st.warning("Aucun th√®me significatif n'a √©t√© g√©n√©r√©. Essayez d'ajuster les param√®tres ou de fournir plus de donn√©es.")


# Configuration de la page
st.set_page_config(page_title="Projet Webscraping", layout="wide")

# Fonctionnalit√©s dans la barre lat√©rale
st.sidebar.title("Fonctionnalit√©s")

if 'page' not in st.session_state:
    st.session_state.page = "home"

if st.sidebar.button("Accueil"):
    st.session_state.page = "home"
if st.sidebar.button("Scraping"):
    st.session_state.page = "scraping"
if st.sidebar.button("Pr√©traitement"):
    st.session_state.page = "preprocessing"
if st.sidebar.button("Analyse des Sentiments"):
    st.session_state.page = "sentiment_analysis"
if st.sidebar.button("Topic Modeling"):
    st.session_state.page = "topic_modeling"

# Affichage de la page correspondante
if st.session_state.page == "home":
    home()
elif st.session_state.page == "scraping":
    scraping()
elif st.session_state.page == "preprocessing":
    preprocessing()
elif st.session_state.page == "sentiment_analysis":
    sentiment_analysis()
elif st.session_state.page == "topic_modeling":
    topic_modeling()